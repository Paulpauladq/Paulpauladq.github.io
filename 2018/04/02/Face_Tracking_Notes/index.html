<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Face Tracking | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Face TrackingFace Detection and Tracking Using the KLT Algorithm 脸部追踪（甚至该人倾斜他的头，移动或者头部离相机很远的时候）  使用feature points  脸部追踪的三步：  面部检测 定义追踪的特征 追踪   首先使用Viola-Jones detection algorithm进行人脸检测，随后使用minimum eig">
<meta property="og:type" content="article">
<meta property="og:title" content="Face Tracking">
<meta property="og:url" content="http://yoursite.com/2018/04/02/Face_Tracking_Notes/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Face TrackingFace Detection and Tracking Using the KLT Algorithm 脸部追踪（甚至该人倾斜他的头，移动或者头部离相机很远的时候）  使用feature points  脸部追踪的三步：  面部检测 定义追踪的特征 追踪   首先使用Viola-Jones detection algorithm进行人脸检测，随后使用minimum eig">
<meta property="og:locale" content="default">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_35.PNG">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/WeChat%20Image_20171221210136.jpg">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/WeChat%20Image_20171221210131.jpg">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/WeChat%20Image_20171221210127.jpg">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/WeChat%20Image_20171221210114.jpg">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_25.PNG">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_35.PNG">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_45.PNG">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_200.PNG">
<meta property="og:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/example1.PNG">
<meta property="og:updated_time" content="2018-04-02T07:55:28.620Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Face Tracking">
<meta name="twitter:description" content="Face TrackingFace Detection and Tracking Using the KLT Algorithm 脸部追踪（甚至该人倾斜他的头，移动或者头部离相机很远的时候）  使用feature points  脸部追踪的三步：  面部检测 定义追踪的特征 追踪   首先使用Viola-Jones detection algorithm进行人脸检测，随后使用minimum eig">
<meta name="twitter:image" content="c:/Users/Paul/Desktop/Face%20Tracking/Images/GFTT/GFTT_35.PNG">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Face_Tracking_Notes" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/02/Face_Tracking_Notes/" class="article-date">
  <time datetime="2018-04-02T07:29:40.888Z" itemprop="datePublished">2018-04-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Face Tracking
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Face-Tracking"><a href="#Face-Tracking" class="headerlink" title="Face Tracking"></a>Face Tracking</h1><h2 id="Face-Detection-and-Tracking-Using-the-KLT-Algorithm"><a href="#Face-Detection-and-Tracking-Using-the-KLT-Algorithm" class="headerlink" title="Face Detection and Tracking Using the KLT Algorithm"></a>Face Detection and Tracking Using the KLT Algorithm</h2><ol>
<li><p>脸部追踪（甚至该人倾斜他的头，移动或者头部离相机很远的时候）</p>
</li>
<li><p>使用feature points</p>
</li>
<li><p>脸部追踪的三步：</p>
<ul>
<li>面部检测</li>
<li>定义追踪的特征</li>
<li>追踪</li>
</ul>
</li>
<li><p>首先使用Viola-Jones detection algorithm进行人脸检测，随后使用minimum eigenvalue算法提取features</p>
</li>
<li><p><strong>Thinking</strong>：</p>
<ul>
<li>Face detection focuses more on the <strong>detection accuracy</strong>, whereas face tracking pays more attention to the detection <strong>time cost</strong> because most face tracker needs to achieve real-time properties.</li>
<li>We can improve the time performance by optimizing each step of face tracking, since we already had the state-of-the-art face detection technics, we narrow down our work to optimizing the last 2 steps.</li>
</ul>
</li>
<li><p><strong>Question</strong>: the classic algorithm and CNN method, which one is better on real-time solution?</p>
<p>​</p>
</li>
</ol>
<h2 id="Viola-amp-Jones-Face-Detection"><a href="#Viola-amp-Jones-Face-Detection" class="headerlink" title="Viola&amp;Jones Face Detection"></a>Viola&amp;Jones Face Detection</h2><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf</a></p>
<h3 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h3><ol>
<li>新方法优点：<ul>
<li>速度极快</li>
<li>准确率极高</li>
</ul>
</li>
<li>主要contribution:<ul>
<li>Integral Image</li>
<li>Adaboost算法提取features</li>
<li>cascade级联结合复杂分类器的方法</li>
</ul>
</li>
</ol>
<h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><ol>
<li>Harr-like features: 双/多矩形feature，计算两/多个相邻矩形像素和的差值</li>
<li>问题所在：太多冗余（无用）features</li>
</ol>
<h3 id="Integral-Image"><a href="#Integral-Image" class="headerlink" title="Integral Image"></a>Integral Image</h3><ol>
<li>Definition: ii(x,y) calculates the sum of pixels above and to the left of x,y, inclusive</li>
<li>The sum with D: 4+1-2-3 (Figure 2)</li>
</ol>
<h3 id="Learning-Classification-Functions"><a href="#Learning-Classification-Functions" class="headerlink" title="Learning Classification Functions"></a>Learning Classification Functions</h3><ol>
<li><p>Motivation: 24 x 24的图片具有超过180000种features，大多数没有用，需要筛选出有用的features</p>
</li>
<li><p>分类学习时使用Adaboost算法:</p>
<ul>
<li><p>初始化训练数据的权值分布，开始时每个样本权值相同。</p>
<p>$D_t(i) = (w_1, w_2, … , w_n) = (1/N,….,1/N)$</p>
</li>
<li><p>对每个特征j，训练一个弱分类器，计算在分布$D_t$上的误差，选择误差最小的弱分类器，更新训练数据寄的权值分布，用于下一轮的迭代(增加误分类样本的权值，减小正确分类样本的权值)</p>
</li>
<li><p>组合各个弱分类器，使用sign函数得到最终分类器</p>
</li>
<li><p>计算复杂度：O(MNT), M filters/ N examples/ T thresholds</p>
</li>
</ul>
</li>
<li><p>Adaboost选出的前几个feature是： 眼睛部分比脸颊部分颜色深</p>
<ul>
<li>眼睛部分比鼻梁部分颜色深</li>
</ul>
</li>
</ol>
<h3 id="Cascading-Classifiers"><a href="#Cascading-Classifiers" class="headerlink" title="Cascading Classifiers"></a>Cascading Classifiers</h3><ol>
<li>级联消除negative samples</li>
</ol>
<h2 id="Good-Features-to-Track"><a href="#Good-Features-to-Track" class="headerlink" title="Good Features to Track"></a>Good Features to Track</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ol>
<li>问题：tracking已经实现，但是选择容易被追踪的特征很难</li>
<li>提出：<ul>
<li>基于tracker工作的特征选择的标准</li>
<li>一个特征检测器</li>
<li>和现实世界无关的特征</li>
</ul>
</li>
</ol>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ol>
<li>SSD: sum-of -squared-difference(SSD) methods ??</li>
<li><p><strong>Feature Dissimilarity</strong>: </p>
<ul>
<li>Definition: the quantity that describes <strong>the changes of appearance of a feature (rms residue)</strong> between the 1st frame and the current frame</li>
<li><strong>RMS RESIDUE</strong>: google look up</li>
<li>Idea: When the dissimilarity grows too large, the feature is a “bad” one. </li>
</ul>
</li>
<li><p><strong>Affine</strong> image changes: linear warping and transforming the image</p>
</li>
<li><p>2 main contributions:</p>
<ul>
<li>Prove that for <strong>measure dissimilarity</strong> : pure translation–NOT ADEQUATE; affine image changes–ADEQUATE</li>
<li>Propose a GOOD way to determine the affine changes</li>
<li>extra: a more PRINCIPLED way to select features</li>
</ul>
</li>
<li><p>Result:</p>
<ul>
<li><p>2 models of image motion &gt; 1model</p>
</li>
<li><p><strong>Dependency</strong>: </p>
<ul>
<li>inter-frame camera translation is small – pure translation</li>
<li>distant frames – affine changes</li>
</ul>
</li>
</ul>
</li>
<li>思考：frame中的物体变换较小，pure translation在节省计算量的情况下，可以保持较为不错的准确度，此时采用affine image changes追踪到的参数不可靠，但是物体移动很快时，affine image changes是必须的</li>
</ol>
<h3 id="Two-Models-of-Image-Motion"><a href="#Two-Models-of-Image-Motion" class="headerlink" title="Two Models of Image Motion"></a><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\GFTT_35.PNG" alt="GFTT_35">Two Models of Image Motion</h3><ol>
<li>pure translation: tracking</li>
<li>affine changes: compare feature to monitor</li>
</ol>
<h3 id="Computing-Image-Motion"><a href="#Computing-Image-Motion" class="headerlink" title="Computing Image Motion"></a>Computing Image Motion</h3><ol>
<li><p>Details can be found in the <strong>derivation process</strong>.</p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\WeChat Image_20171221210136.jpg" alt="WeChat Image_20171221210136"></p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\WeChat Image_20171221210131.jpg" alt="WeChat Image_20171221210131"></p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\WeChat Image_20171221210127.jpg" alt="WeChat Image_20171221210127"></p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\WeChat Image_20171221210114.jpg" alt="WeChat Image_20171221210114"></p>
<p>​</p>
</li>
<li><p>We divide all the circumstances into two separate parts:</p>
<ul>
<li>During tracking, the affine deformation D of the feature window is small, so we set D to 0 matrix. In such cases, we solve Zd = e.</li>
<li>During feature monitoring, motion is too large to be described well by pure translation, the entire transformation Tz = a needs to be solved.</li>
</ul>
</li>
</ol>
<h3 id="Texturedness"><a href="#Texturedness" class="headerlink" title="Texturedness"></a>Texturedness</h3><ol>
<li>Problems aimed to be solved: motions are not <strong>complete</strong></li>
<li>Typical solution: track corners or windows with a high spatial frequency content(????), or regions where second-order derivatives is high(??????). But there exists 2 problems:<ul>
<li>Intuitive and preconceived, their methods aren’t guarantee to be best for tracking</li>
<li>usually defined for the pure translation model, hard to extend them to affine motion</li>
</ul>
</li>
<li>PRINCIPLED DEFINITION OF <strong>FEATURE QUALITY</strong></li>
<li>(????????) 对于Z的两个特征值，Accept a window if: $min(\lambda_1, \lambda_2 ) &gt; \lambda$</li>
</ol>
<h3 id="Dissimilarity"><a href="#Dissimilarity" class="headerlink" title="Dissimilarity"></a>Dissimilarity</h3><h3 id="Simulations"><a href="#Simulations" class="headerlink" title="Simulations"></a>Simulations</h3><h3 id="Python-Implementation"><a href="#Python-Implementation" class="headerlink" title="Python Implementation"></a>Python Implementation</h3><ol>
<li><p>A simple demo of python:选择一副图片，使用opencv的good features to track算法可以清楚的提取出高质量的特征点，这里选择如下的含有明显“好的”（棱角分明）特征的图片，便于观察效果，使用python实现算法，并且分别设置获取特征点的个数为25,35,45,200:</p>
<p>​</p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\GFTT_25.PNG" alt="GFTT_25"></p>
<p>​</p>
</li>
</ol>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\GFTT_35.PNG" alt="GFTT_35"></p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\GFTT_45.PNG" alt="GFTT_45"></p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\GFTT_200.PNG" alt="GFTT_200"></p>
<p>由上面的实验结果可以观察到特征提取的结果较为良好，直观来看提取的点都在边缘上。</p>
<ul>
<li>思考：<ol>
<li>后续的实验可以采用规范化的特征来评估“好”的程度，便于量化goodness，以及便于和其他优化算法比对。</li>
<li>使用Python的实现，只能在图片上显示track的效果，但不能清楚地看到每个被提取点的”Goodness”值</li>
</ol>
</li>
</ul>
<h3 id="A-implementation-of-the-KLT-Feature-Tracker-C"><a href="#A-implementation-of-the-KLT-Feature-Tracker-C" class="headerlink" title="A implementation of the KLT Feature Tracker(C++)"></a>A implementation of the KLT Feature Tracker(C++)</h3><ol>
<li><p>Home page: <a href="http://cecas.clemson.edu/~stb/klt/" target="_blank" rel="noopener">http://cecas.clemson.edu/~stb/klt/</a></p>
</li>
<li><p>这是网上已经实现好的KLT tracker，解决了上面自己实现的问题，在Linux上运行可以选择一副图片提取特征点并且可以按照每个特征点的goodness值打印出所有被提取点的坐标，结果如下：</p>
<p><img src="C:\Users\Paul\Desktop\Face Tracking\Images\GFTT\example1.PNG" alt="example1"></p>
</li>
</ol>
<h2 id="An-Iterative-Image-Registration-Technique-with-an-Application-to-Stereo-Vision"><a href="#An-Iterative-Image-Registration-Technique-with-an-Application-to-Stereo-Vision" class="headerlink" title="An Iterative Image Registration Technique with an Application to Stereo Vision"></a>An Iterative Image Registration Technique with an Application to Stereo Vision</h2><h3 id="The-Registration-Problem"><a href="#The-Registration-Problem" class="headerlink" title="The Registration Problem"></a>The Registration Problem</h3><ol>
<li>Problem: F(x) and G(x) gives the corresponding pixel value for location x in two different images. Our goal is to find out the <strong>disparity</strong> vector h that minimizes of the difference between F(x + h) and G(x) in Region of Interest R</li>
<li>Typical Solution: $L_1 norm$, $L_2 norm$, negative of normalized correlation</li>
</ol>
<h3 id="The-Registration-Algorithm"><a href="#The-Registration-Algorithm" class="headerlink" title="The Registration Algorithm"></a>The Registration Algorithm</h3><ol>
<li>One-D case</li>
<li>2D Case</li>
</ol>
<h2 id="The-First-Facial-Landmark-Tracking-in-the-Wild-Challenge-Benchmark-and-Results"><a href="#The-First-Facial-Landmark-Tracking-in-the-Wild-Challenge-Benchmark-and-Results" class="headerlink" title="The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results"></a>The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results</h2><h2 id="High-Speed-Tracking-with-Kernelized-Correlation-Filters"><a href="#High-Speed-Tracking-with-Kernelized-Correlation-Filters" class="headerlink" title="High-Speed Tracking with Kernelized Correlation Filters"></a>High-Speed Tracking with Kernelized Correlation Filters</h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><ol>
<li>重大突破：区别学习方法 – 再有目标的image patch中区分出目标的外貌和背景的外貌</li>
<li>Motivation: <ul>
<li>一个样本中又无限数量的负样本，为了平衡时间和样本数量，一般的做法是再每一帧随机选取几个样本</li>
<li>undersampling会大大降低性能</li>
</ul>
</li>
</ol>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><ol>
<li>tracking-by-detection：</li>
<li>On sample translation and correlation filtering </li>
</ol>
<h3 id="Building-Blocks"><a href="#Building-Blocks" class="headerlink" title="Building Blocks"></a>Building Blocks</h3><ol>
<li>Linear regression(线性回归训练提速):   </li>
</ol>
<p>###Code Analysis </p>
<ol>
<li><p>choose_video.m: </p>
<ul>
<li>功能：选择视频的种类</li>
</ul>
</li>
</ol>
<ul>
<li>参数： base_path（视频路径）</li>
<li>函数实现细节：<ul>
<li>确认视频路径统一</li>
<li>列出所有子目录</li>
<li>显示GUI，让用户选择</li>
</ul>
</li>
</ul>
<ol start="2">
<li><p>download_videos.m: 下载视频，解压到固定路径</p>
</li>
<li><p>fhog.m: </p>
<ul>
<li>功能：计算FHOG（改良版HOG特征）</li>
<li>输入参数：<ul>
<li>I: 输入图片</li>
<li>binSize: bin的大小</li>
<li>nOrients: 有方向的bin的个数</li>
<li>clip: 剪切</li>
<li>crop: 是否剪辑边缘</li>
</ul>
</li>
<li>输出：计算出的函数</li>
</ul>
</li>
<li><p>gaussian_correlation.m: </p>
<ul>
<li>功能：计算高斯内核函数</li>
<li>输入参数：<ul>
<li>xf,yf: 傅里叶空间中的输入图片x,y（都是m x n）</li>
<li>sigma: bandwidth（常参数）</li>
</ul>
</li>
<li>输出：一个m x n的响应图</li>
</ul>
</li>
<li><p>gaussian_shaped_labels.m:</p>
<ul>
<li>功能：为每个样本的所有shifts建立一个标签队列（回归目标）</li>
<li>输入参数：<ul>
<li>sigma: const</li>
<li>sz: 标签数组大小</li>
</ul>
</li>
<li>输出：大小为SZ（shift数量）的标签数组</li>
</ul>
</li>
<li><p>get_features.m:</p>
<ul>
<li>功能：从图片中提取特征</li>
<li>输入参数：<ul>
<li>im: 输入图片</li>
<li>features: 采集的特征（HOG特征或者是灰度特征）</li>
<li>cell_size: cell的大小</li>
<li>cos_window: 添加汉宁窗使得图像边缘平滑</li>
</ul>
</li>
<li>输出：数组x – [cell的高度， cell的宽度，特征向量]</li>
</ul>
</li>
<li><p>get_subwindows.m:</p>
<ul>
<li>功能：获得子窗口</li>
<li>输入参数:<ul>
<li>im: 提取的图片</li>
<li>pos: 子窗口的中心</li>
<li>sz: 子窗口的长宽（如果子窗口到了图片外面则复制边缘像素来填充空白）</li>
</ul>
</li>
<li>输出：提取出的子窗口区域(ys ,xs ,:)</li>
</ul>
</li>
<li><p>linear_correlation.m:</p>
<ul>
<li>功能: 计算输入线性内核函数</li>
<li>输入参数：xf,yf: 傅里叶空间中的输入图片x,y（都是m x n）</li>
<li>输出：m x n的响应图谱</li>
</ul>
</li>
<li><p>load_video_info.m:</p>
<ul>
<li>功能：加载指定路径中的所有视频的相关性信息</li>
<li>输入参数：视频存放路径，视频名称</li>
<li>输出：各种相关信息<ul>
<li>img_files: 图片文件的数组</li>
<li>pos: 起始位置坐标</li>
<li>target_sz: 目标大小</li>
<li>ground_truth: N帧中gt的位置</li>
<li>video_path: 视频存放的路径</li>
</ul>
</li>
</ul>
</li>
<li><p>polynomial_correlation.m:</p>
<ul>
<li>功能：在所有shifts上计算多项式内核</li>
<li>输入参数：<ul>
<li>xf,yf: m x n的输入图像</li>
<li>a: 多项式内核的常数</li>
<li>b: 多项式内核的指数</li>
</ul>
</li>
<li>输出：一个m x n的响应图</li>
</ul>
</li>
<li><p>precision_plot.m: </p>
<ul>
<li>功能：绘制precision/threshold的图形曲线</li>
<li>输入参数：<ul>
<li>positions: 在每一帧（N帧）预测的位置（N * 2） </li>
<li>ground_truth: 在每一帧（N帧）实际的位置（N * 2） </li>
<li>title: 短视频的名称</li>
<li>show: boolean值 – 是否显示曲线</li>
</ul>
</li>
</ul>
</li>
<li><p>run_tracker.m:</p>
<ul>
<li>功能：KCF/DCF的主要接口（设置参数，加载视频，计算准确率以及fps）</li>
<li>输入参数：<ul>
<li>video: 视频类别</li>
<li>kernel_type: 内核类别</li>
<li>feature_type: 特征类别</li>
<li>show_visualization: 是否显示下拉菜单</li>
<li>show_plots: 是否显示曲线图 </li>
</ul>
</li>
<li>输出：无</li>
</ul>
</li>
<li><p>show_video.m:</p>
<ul>
<li>功能：可视化视频段</li>
<li>输入参数：<ul>
<li>img_files: 每一帧的图片文件名称</li>
<li>video_path: 视频路径</li>
<li>resize_image: 是否重置图片大小</li>
</ul>
</li>
<li>输出：UPDATE_VISUALIZATION的函数handle</li>
</ul>
</li>
<li><p>tracker.m:</p>
<ul>
<li><p>功能：实现了kcf/dcf的基本流程</p>
</li>
<li><p>输入参数：</p>
<ul>
<li>video_path: 图片信息被存储的位置</li>
<li>img_files: 是每张图片名称的数组</li>
<li>pos: 目标的起始位置</li>
<li>target_sz: 目标的起始大小</li>
<li>padding: 增加的被追踪的区域相对原本目标的<strong>尺寸</strong></li>
<li>kernel: 决定使用内核种类的一个结构数组（“类型”，“sigma”，“poly_a”，“poly_b”）</li>
<li>lambda: 归一化参数</li>
<li>output_sigma_factor: spatial bandwidth（与目标大小有关）</li>
<li>interp_factor: 追踪器的适应率？？？？</li>
<li>cell_size: 每个cell的像素数量（如果是raw pixel则为1）</li>
<li>features: 决定使用特征种类的一个结构数组</li>
<li>show_visualization: boolean值决定是否显示可交互的视频</li>
</ul>
<p>​</p>
</li>
<li><p>输出：</p>
<ul>
<li>positions: 每一帧的目标位置（N x 2的矩阵）</li>
<li>time: tracker执行的时间</li>
</ul>
</li>
<li><p>实现细节：</p>
<ul>
<li>如果目标个太大，改变图片的尺寸，降低清晰度</li>
<li>设置追踪窗口大小（目标大小加上padding的大小）</li>
<li>使用gaussian_shaped_labels创建循环样本回归的标签yf</li>
<li>使用汉宁窗处理样本</li>
<li>创建视频接口</li>
<li>对于一个视频的每一帧进行循环：读取图片，预处理图片</li>
<li>对于第一帧，选取内核种类，使用单一图片训练模型的系数alphaf（使用kernel计算，按照快速训练公式eq.17）</li>
<li>从第二帧开始取前一帧的子窗口，提取体征并转换到傅里叶空间–zf</li>
<li>对于傅氏空间的特征向量进行内核计算，得出每个shifts的response</li>
<li>对于response进行2d逆傅里叶变换到实数空间，并找到其最大值即检测目标中心点，更新目标位置</li>
<li>使用前一帧的model_alphaf以及model_xf结合新得到的值，更新模型alphaf以及xf</li>
<li>存储每一帧的预测位置和所用时间</li>
</ul>
</li>
</ul>
</li>
<li><p>videofig.m</p>
<ul>
<li>功能：创建一个有水平进度条以及各种快捷键的图像</li>
</ul>
</li>
<li><p>Q&amp;A：</p>
<ul>
<li>response wrap around不明白</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">%target location is at the maximum response. we must take into</span><br><span class="line">%account the fact that, if the target doesn&apos;t move, the peak</span><br><span class="line">%will appear at the top-left corner, not at the center (this is</span><br><span class="line">%discussed in the paper). the responses wrap around cyclically.</span><br><span class="line">[vert_delta, horiz_delta] = find(response == max(response(:)), 1);</span><br><span class="line">if vert_delta &gt; size(zf,1) / 2,  %wrap around to negative half-space of vertical axis</span><br><span class="line">	vert_delta = vert_delta - size(zf,1);</span><br><span class="line">end</span><br><span class="line">if horiz_delta &gt; size(zf,2) / 2,  %same for horizontal axis</span><br><span class="line">	horiz_delta = horiz_delta - size(zf,2);</span><br><span class="line">end</span><br><span class="line">pos = pos + cell_size * [vert_delta - 1, horiz_delta - 1];</span><br></pre></td></tr></table></figure>
<ul>
<li><p>如何生成response map的？</p>
<p>eq.22</p>
</li>
<li><p>labels标注问题！！！！！</p>
<ul>
<li>使用的是高斯label</li>
<li>对于一个短视频的每一帧图片，将它循环平移（水平方向，竖直方向都有），平移间隔为cell宽度（如果是raw pixels则为1)，所以生成的高斯标签为m x n大小，（m x n即原图中的cell个数）</li>
<li>需要将其中响应最高的点移到左上角，方便之后计运算</li>
</ul>
</li>
</ul>
<p>​</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/02/Face_Tracking_Notes/" data-id="cjfrnjez90000isvjbgsbv0tv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/04/02/Hadoop/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2018/04/02/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello PPT</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/04/09/fuck/">fuck</a>
          </li>
        
          <li>
            <a href="/2018/04/02/Machine Learning by Andrew NG/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/04/02/Hadoop/">(no title)</a>
          </li>
        
          <li>
            <a href="/2018/04/02/Face_Tracking_Notes/">Face Tracking</a>
          </li>
        
          <li>
            <a href="/2018/04/02/hello-world/">Hello PPT</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>